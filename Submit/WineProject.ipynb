{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9WaynNodC7X"
   },
   "source": [
    "# Wine quality predictions with machine learning\n",
    "\n",
    "### Authors:\n",
    "Armin Shafiee, Fernando Moyano, Loay Ahmed Elalfy Abdelhafiz, Alexander Wemhoff "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TZozBkwHXlA"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Op2DSJmsXyeO"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Common imports\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "# %matplotlib inline\n",
    "\n",
    "\n",
    "# Setup of directories and options\n",
    "ROOT_DIR = \".\"\n",
    "IMAGES_DIR = os.path.join(ROOT_DIR, \"images\")\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
    "\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "WINE_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wine_raw_filepath = os.path.join(DATA_DIR, \"winequality-red_raw.csv\")\n",
    "wine_filepath = os.path.join(DATA_DIR, \"winequality-red.csv\")\n",
    "\n",
    "make_plots = False\n",
    "GridSearch = False\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Some useful functions ----\n",
    "\n",
    "# Save figures function\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_DIR, fig_id + \".\" + fig_extension)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "\n",
    "# Download data function\n",
    "def fetch_data(data_url, file_path):\n",
    "    if os.path.isfile(file_path):\n",
    "        print('File already exists.')\n",
    "    else:\n",
    "        urllib.request.urlretrieve(data_url, file_path)\n",
    "\n",
    "\n",
    "# Recode csv file function\n",
    "def recode_data(filein, fileout):\n",
    "    fString = open(filein, \"r\")\n",
    "    fFloat = open(fileout, \"w\")\n",
    "    for line in fString:\n",
    "        line = line.replace(\";\", \",\")\n",
    "        # line = line.replace(\"\\t\", \",\")\n",
    "        # line = line.replace(\"\\r\\n\", \"\\n\")\n",
    "        fFloat.write(line)\n",
    "    fString.close()\n",
    "    fFloat.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJblEresE6UT"
   },
   "source": [
    "## Data aquisition and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZXT3yfeX_x2",
    "outputId": "0ec279df-bf78-432a-f55a-67149be8a3e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fix_acid  volat_acid  citric_acid  ...  sulphates  alcohol  quality\n",
      "0       7.4        0.70         0.00  ...       0.56      9.4        5\n",
      "1       7.8        0.88         0.00  ...       0.68      9.8        5\n",
      "2       7.8        0.76         0.04  ...       0.65      9.8        5\n",
      "3      11.2        0.28         0.56  ...       0.58      9.8        6\n",
      "4       7.4        0.70         0.00  ...       0.56      9.4        5\n",
      "\n",
      "[5 rows x 12 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   fix_acid        1599 non-null   float64\n",
      " 1   volat_acid      1599 non-null   float64\n",
      " 2   citric_acid     1599 non-null   float64\n",
      " 3   resid_sugar     1599 non-null   float64\n",
      " 4   chlorides       1599 non-null   float64\n",
      " 5   free_sulf_diox  1599 non-null   float64\n",
      " 6   tot_sulf_diox   1599 non-null   float64\n",
      " 7   density         1599 non-null   float64\n",
      " 8   pH              1599 non-null   float64\n",
      " 9   sulphates       1599 non-null   float64\n",
      " 10  alcohol         1599 non-null   float64\n",
      " 11  quality         1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n",
      "None\n",
      "          fix_acid   volat_acid  ...      alcohol      quality\n",
      "count  1599.000000  1599.000000  ...  1599.000000  1599.000000\n",
      "mean      8.319637     0.527821  ...    10.422983     5.636023\n",
      "std       1.741096     0.179060  ...     1.065668     0.807569\n",
      "min       4.600000     0.120000  ...     8.400000     3.000000\n",
      "25%       7.100000     0.390000  ...     9.500000     5.000000\n",
      "50%       7.900000     0.520000  ...    10.200000     6.000000\n",
      "75%       9.200000     0.640000  ...    11.100000     6.000000\n",
      "max      15.900000     1.580000  ...    14.900000     8.000000\n",
      "\n",
      "[8 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get the data ----\n",
    "fetch_data(WINE_URL, wine_raw_filepath)\n",
    "recode_data(wine_raw_filepath, wine_filepath)\n",
    "wine = pd.read_csv(wine_filepath)\n",
    "# Change names for better plotting\n",
    "wine.columns = ['fix_acid', 'volat_acid', 'citric_acid', 'resid_sugar',\n",
    "                'chlorides', 'free_sulf_diox', 'tot_sulf_diox', 'density',\n",
    "                'pH', 'sulphates', 'alcohol', 'quality']\n",
    "\n",
    "# Display descriptive info and statistics ----\n",
    "print(wine.head())\n",
    "print(wine.info())\n",
    "print(wine.describe())\n",
    "\n",
    "# Create histograms to check data distributions ----\n",
    "if make_plots:\n",
    "    wine.hist(bins=50, figsize=(20, 15))\n",
    "    save_fig(\"attribute_histogram_plots\")\n",
    "    plt.show()\n",
    "\n",
    "# Correlations ----\n",
    "corr_matrix = wine.corr()\n",
    "corr_matrix['quality'].sort_values(ascending=False)\n",
    "\n",
    "# Create a scatter matrix to check for correlations between variables\n",
    "# Removed weaker correlations variables: 'resid_sugar', 'chlorides', 'free_sulf_diox', 'tot_sulf_diox', 'quality'\n",
    "if make_plots:\n",
    "    attributes = ['fix_acid', 'volat_acid', 'citric_acid', 'density', 'pH', 'sulphates', 'alcohol']\n",
    "    scatter_matrix(wine[attributes], figsize=(28, 28))\n",
    "    save_fig(\"scatter_matrix_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcnGW3hjFEJe"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cMkKhV_-YExS"
   },
   "outputs": [],
   "source": [
    "# Split into features and target variables\n",
    "X = wine.iloc[:, 0:11]\n",
    "y = wine.iloc[:, 11]\n",
    "features = X.columns\n",
    "\n",
    "# Split into training and testing by stratifying using the target variable ('quality')\n",
    "# Necessary because of very few low and high quality samples\n",
    "X_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Testing scaling options\n",
    "# # Normalizing lognormal data: option set to also standardize to 0 mean and unit sd\n",
    "# # https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "powertrans = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "X_test = powertrans.fit_transform(X_trn)\n",
    "\n",
    "# Apply initial PCA to explore components\n",
    "pca = PCA(n_components=11)\n",
    "pca.fit(X_test)\n",
    "pca_evr = pca.explained_variance_ratio_  # The first two components explain most of the variance.\n",
    "pca_ev = pca.explained_variance_\n",
    "\n",
    "# Plot cumulative sums of explained variance to decide on number of components\n",
    "if make_plots:\n",
    "    plt.subplot(121)\n",
    "    plt.plot(np.concatenate(([0], np.cumsum(pca_evr))))\n",
    "    plt.subplot(122)\n",
    "    plt.plot(np.concatenate(([0], np.cumsum(pca_ev))))\n",
    "    save_fig(\"expvar_pca\")\n",
    "\n",
    "# Making a Pipeline for the preprocessing\n",
    "# We use 6 components for PCA to keep about 80% of the explained variance.\n",
    "\n",
    "# Only scaling pipeline\n",
    "s_pipeline = Pipeline([\n",
    "        ('scaler', PowerTransformer(method='yeo-johnson', standardize=True)),\n",
    "    ])\n",
    "\n",
    "# Only PCA pipeline\n",
    "p_pipeline = Pipeline([\n",
    "        ('pca', PCA(n_components=6)),\n",
    "    ])\n",
    "\n",
    "# Scaling a PCA pipeline\n",
    "sp_pipeline = Pipeline([\n",
    "        ('scaler', PowerTransformer(method='yeo-johnson', standardize=True)),\n",
    "        ('pca', PCA(n_components=6))\n",
    "    ])\n",
    "\n",
    "# Fit and transform on train data\n",
    "X_trn_s = s_pipeline.fit_transform(X_trn)\n",
    "X_trn_p = p_pipeline.fit_transform(X_trn)\n",
    "X_trn_sp = sp_pipeline.fit_transform(X_trn)\n",
    "# Transform only on test data\n",
    "X_tst_s = s_pipeline.transform(X_tst)\n",
    "X_tst_p = p_pipeline.transform(X_tst)\n",
    "X_tst_sp = sp_pipeline.transform(X_tst)\n",
    "\n",
    "X_list = list(zip([\"X_trn\", \"X_trn_s\", \"X_trn_sp\"], [X_trn, X_trn_s, X_trn_sp]))\n",
    "\n",
    "# Look at distributions after scaling\n",
    "if make_plots:\n",
    "    pd.DataFrame(X_trn_s).hist(bins=50, figsize=(20, 15))\n",
    "    save_fig(\"xtrain_histogram_plots\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFLaoAXchVn0"
   },
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMGPyPGohVn1",
    "outputId": "7c0e2105-0b33-4922-c6ab-eadfbd53a99f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trn 0.5949960906958561 0.5886927280369904\n",
      "X_trn_s 0.6129788897576232 0.6098043252609997\n",
      "X_trn_sp 0.5973416731821736 0.5902454298238842\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(tol=0.0001, C=0.1, solver='newton-cg')\n",
    "for names, X_pp in X_list:\n",
    "  log_reg.fit(X_pp, y_trn)\n",
    "  log_score = log_reg.score(X_pp, y_trn)\n",
    "  log_cross_scores = np.mean(cross_val_score(log_reg, X_pp, y_trn, cv=7))\n",
    "  print(names, log_score, log_cross_scores)\n",
    "\n",
    "# Using best X dataset\n",
    "log_reg.fit(X_trn_s, y_trn)\n",
    "\n",
    "best_logreg_clf = log_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxRKWzKZoDzA"
   },
   "source": [
    "## K Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZsVUdehomcF"
   },
   "outputs": [],
   "source": [
    "for i in range(1,9):\n",
    "  for names, X_pp in X_list:\n",
    "    neigh = KNeighborsClassifier(n_neighbors=i)\n",
    "    neigh.fit(X_pp, y_trn)\n",
    "    knn_score = neigh.score(X_pp, y_trn)\n",
    "    knn_cross_scores = np.mean(cross_val_score(neigh, X_pp, y_trn, cv=7))\n",
    "    print(names, i, knn_score, knn_cross_scores)\n",
    "\n",
    "param_grid = [{'n_neighbors': [2,3,4,5,6,7,8], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}]\n",
    "neigh = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(neigh, param_grid, cv=7, return_train_score=True)\n",
    "grid_search.fit(X_trn_s, y_trn)\n",
    "\n",
    "grid_search.best_params_\n",
    "\n",
    "# KNN with best parameters from grid search: K=5 kein PCA\n",
    "neigh_gs = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh_gs.fit(X_trn_s, y_trn)\n",
    "knn_score = neigh_gs.score(X_trn_s, y_trn)\n",
    "knn_cross_scores = np.mean(cross_val_score(neigh_gs, X_trn_s, y_trn, cv=7))\n",
    "print(knn_score,knn_cross_scores)\n",
    "\n",
    "best_knn_clf = neigh_gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOvFyztisQNy"
   },
   "source": [
    "## Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMni9jxqsOls",
    "outputId": "de7781d0-9d24-43bb-cc5d-2a23b8d35c7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial test score for X_trn: 0.5152462861610634\n",
      "Initial test score for X_trn_s: 0.6888193901485535\n",
      "Initial test score for X_trn_sp: 0.6442533229085223\n",
      "Cross validation score mean for X_trn: 0.5074255485893416\n",
      "Cross validation score mean for X_trn_s: 0.6129726684952977\n",
      "Cross validation score mean for X_trn_sp: 0.5996865203761755\n"
     ]
    }
   ],
   "source": [
    "# First test with default options\n",
    "svc = SVC()\n",
    "\n",
    "# Fit on different X datasets\n",
    "for X_name, X in X_list:\n",
    "    svc.fit(X, y_trn)\n",
    "    print(f\"Initial test score for {X_name}: {svc.score(X, y_trn)}\")\n",
    "\n",
    "# Cross validation on different datasets\n",
    "for X_name, X in X_list:\n",
    "    svc_scores = cross_val_score(svc, X, y_trn, cv=4, scoring=\"accuracy\")\n",
    "    print(f\"Cross validation score mean for {X_name}: {np.mean(svc_scores)}\")\n",
    "\n",
    "# Best scores with scaled values: X_trn_s\n",
    "\n",
    "if GridSearch:\n",
    "    # Grid search\n",
    "    param_grid = {'C': [0.1, 1, 10],\n",
    "                  'gamma': ['scale', 'auto'],\n",
    "                  'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "                  'decision_function_shape': ['ovo', 'ovr'],}\n",
    "\n",
    "    svc_grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=1)\n",
    "    svc_grid.fit(X_trn_s, y_trn)\n",
    "    print(\"Best Hyper Parameters for SVC grid search:\\n\", svc_grid.best_params_)\n",
    "\n",
    "    # Cross validate using best parameter set\n",
    "    svc_gs = SVC(C=1, gamma='scale', kernel='rbf', decision_function_shape='ovo')\n",
    "    svc_scores = cross_val_score(svc_gs, X_trn_s, y_trn, cv=4, scoring=\"accuracy\")\n",
    "    print(f\"Cross validation score mean for X_trn_s: {np.mean(svc_scores)}\")\n",
    "\n",
    "# No improvement in score after grid search. Using default.\n",
    "svc = SVC(probability=True)\n",
    "svc.fit(X_trn_s, y_trn)\n",
    "best_svc_clf = svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RoKFkk3_Bag"
   },
   "source": [
    "## Multilayer Perceptron (Neural Network Estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mtwx-J5xlCkf",
    "outputId": "0ebb9511-d469-4269-9f9a-cd015ca1b849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deg_freedom = 280\n",
      "Initial test score for X_trn: 0.5707584050039093\n",
      "Initial test score for X_trn_s: 0.7412040656763096\n",
      "Initial test score for X_trn_sp: 0.673182173573104\n",
      "Initial test cross validation score mean for X_trn: 0.5871546826018809\n",
      "Initial test cross validation score mean for X_trn_s: 0.5824769788401254\n",
      "Initial test cross validation score mean for X_trn_sp: 0.5660413401253919\n",
      "Score after using best pars of grid search: 0.9890539483971853\n",
      "Mean cross validation score using best pars of grid search: 0.601239224137931\n"
     ]
    }
   ],
   "source": [
    "# Initial test using 3 hidden layers and neurons = number of features ----\n",
    "\n",
    "# Degrees of freedom with a maximum of 3 hidden layers\n",
    "nl1 = 10\n",
    "nl2 = 10\n",
    "nl3 = 10\n",
    "print(f\"deg_freedom = {7 * nl1 + nl1 * nl2 + nl2 * nl3 + nl3 * 1}\")\n",
    "\n",
    "mlp_init = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=500,\n",
    "                    learning_rate_init=0.01, learning_rate='adaptive', random_state=2)\n",
    "\n",
    "# Fit on different X datasets\n",
    "for X_name, X in X_list:\n",
    "    mlp_init.fit(X, y_trn)\n",
    "    print(f\"Initial test score for {X_name}: {mlp_init.score(X, y_trn)}\")\n",
    "\n",
    "# Common problem: optimization does not converge. Address later with grid search.\n",
    "\n",
    "for X_name, X in X_list:\n",
    "    mlp_scores = cross_val_score(mlp_init, X, y_trn, cv=4, scoring=\"accuracy\")\n",
    "    print(f\"Initial test cross validation score mean for {X_name}: {np.mean(mlp_scores)}\")\n",
    "\n",
    "# Grid search for best parameters (using only X_trn_s)\n",
    "if GridSearch:\n",
    "    param_grid = [\n",
    "            {\n",
    "                'activation': ['tanh', 'relu'],\n",
    "                'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "                'learning_rate': ['invscaling', 'adaptive'],\n",
    "                'hidden_layer_sizes': [(10, 10), (20, 20), (30, 30), (30, 10), (10, 10, 10), (30, 20, 10)],\n",
    "                'learning_rate_init': [0.01, 0.1],\n",
    "                'max_iter': [300]\n",
    "            }\n",
    "          ]\n",
    "    mlp = MLPClassifier(random_state=1)\n",
    "    start = time.time()\n",
    "    mlp_grid = GridSearchCV(mlp, param_grid, cv=4, scoring='accuracy')\n",
    "    grid_result = mlp_grid.fit(X_trn_s, y_trn)\n",
    "    end = time.time()\n",
    "    print(f\"Elapsed time: {end - start}\")\n",
    "    print(grid_result)\n",
    "    print(\"Best Hyper Parameters:\\n\", mlp_grid.best_params_)\n",
    "\n",
    "# Cross validation with best parameters of first grid search\n",
    "mlp_gs = MLPClassifier(hidden_layer_sizes=(20, 20), activation='tanh',\n",
    "                         learning_rate_init = 0.01, learning_rate='invscaling',\n",
    "                         solver='adam', random_state=1, max_iter=1000)\n",
    "mlp_gs.fit(X_trn_s, y_trn)\n",
    "print(f\"Score after using best pars of grid search: {mlp_gs.score(X_trn_s, y_trn)}\")\n",
    "\n",
    "mlp_gs_scores = cross_val_score(mlp_gs, X_trn_s, y_trn, cv=4, scoring=\"accuracy\")\n",
    "print(f\"Mean cross validation score using best pars of grid search: {np.mean(mlp_gs_scores)}\")\n",
    "\n",
    "best_mlp_clf = mlp_gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tcBxDBGwKU8"
   },
   "source": [
    "## DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTYQBUNowKFv"
   },
   "outputs": [],
   "source": [
    "L1 = [\"X_trn\", \"X_trn_s\"]\n",
    "L2 = [X_trn, X_trn_s]\n",
    "L12 = list(zip(L1, L2))\n",
    "L3 = [\"X_trn_p\", \"X_trn_sp\"]\n",
    "L4 = [X_trn_p, X_trn_sp]\n",
    "L34 = list(zip(L3, L4))\n",
    "L5 = L1 + L3\n",
    "L6 = L2 + L4\n",
    "L7 = [X_tst, X_tst_s, X_tst_p, X_tst_sp]\n",
    "L8 = []\n",
    "L9 = []\n",
    "L10 = []\n",
    "L_tree_test_scores = []\n",
    "L_forest_test_scores = []\n",
    "\n",
    "\n",
    "# def display_scores(scores):\n",
    "    # print(\"Scores:\", scores)\n",
    "    # print(\"Mean training score:\", scores.mean())\n",
    "    # print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "\"\"\"******************** DecisionTreeClassifier ********************\"\"\"\n",
    "\n",
    "# \"\"\"DecisionTreeClassifier without GridSearchCV\"\"\"\n",
    "# L567 = list(zip(L5, L6, L7))\n",
    "# for i, j, k in L567:\n",
    "#     tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "#     tree_clf.fit(j, y_trn)\n",
    "#     # y_pred = tree_clf.predict(j)\n",
    "#     # tree_mse = mean_squared_error(y_trn, y_pred)\n",
    "#     # tree_rmse = np.sqrt(tree_mse)\n",
    "#     # print(tree_rmse)\n",
    "#     tree_mse_scores = cross_val_score(tree_clf, j, y_trn,\n",
    "#                              scoring=\"neg_mean_squared_error\", cv=8)\n",
    "#     tree_rmse_scores = np.sqrt(-tree_mse_scores)\n",
    "#     # display_scores(tree_rmse_scores)\n",
    "#     print(f\"Mean tree training score for {i}: {tree_rmse_scores.mean()}\")\n",
    "#     tree_test_score = tree_clf.fit(j, y_trn).score(k, y_tst)\n",
    "#     print(f\"Tree test score for {i}: {tree_test_score}\")\n",
    "\n",
    "\"\"\"GridSearchCV for tree without PCA\"\"\"\n",
    "if GridSearch:\n",
    "    for i, j in L12:\n",
    "        param_grid = [\n",
    "            # try 10 hyperparameters\n",
    "            {'max_features': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}\n",
    "          ]\n",
    "        tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "        # train across 8 folds, that's a total of 10*8=80 rounds of training\n",
    "        grid_search = GridSearchCV(tree_clf, param_grid, cv=8,\n",
    "                                   scoring='neg_mean_squared_error',\n",
    "                                   return_train_score=True)\n",
    "        grid_search.fit(j, y_trn)\n",
    "        L8.append(grid_search.best_params_['max_features'])\n",
    "        print(f\"The best tree parameters for {i}: {grid_search.best_estimator_}\")\n",
    "\n",
    "    \"\"\"GridSearchCV for tree with PCA\"\"\"\n",
    "    for i, j in L34:\n",
    "        param_grid = [\n",
    "            # try 5 hyperparameters\n",
    "            {'max_features': [2, 3, 4, 5, 6]}\n",
    "          ]\n",
    "        tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "        # train across 8 folds, that's a total of 5*8=40 rounds of training\n",
    "        grid_search = GridSearchCV(tree_clf, param_grid, cv=8,\n",
    "                                   scoring='neg_mean_squared_error',\n",
    "                                   return_train_score=True)\n",
    "        grid_search.fit(j, y_trn)\n",
    "        L8.append(grid_search.best_params_['max_features'])\n",
    "        print(f\"The best tree parameters for {i}: {grid_search.best_estimator_}\")\n",
    "\n",
    "    \"\"\"DecisionTreeClassifier after GridSearchCV for all X-variations\"\"\"\n",
    "    L5678 = list(zip(L5, L6, L7, L8))\n",
    "    for i, j, k, l in L5678:\n",
    "        tree_clf = DecisionTreeClassifier(max_features=l, random_state=42)\n",
    "        tree_clf.fit(j, y_trn)\n",
    "        # y_pred = tree_clf.predict(j)\n",
    "        # tree_mse = mean_squared_error(y_trn, y_pred)\n",
    "        # tree_rmse = np.sqrt(tree_mse)\n",
    "        # print(tree_rmse)\n",
    "        tree_mse_scores = cross_val_score(tree_clf, j, y_trn,\n",
    "                                 scoring=\"neg_mean_squared_error\", cv=8)\n",
    "        tree_rmse_scores = np.sqrt(-tree_mse_scores)\n",
    "        # display_scores(tree_rmse_scores)\n",
    "        print(f\"\\nMean tree training score for {i}: {tree_rmse_scores.mean()}\")\n",
    "        tree_test_score = tree_clf.fit(j, y_trn).score(k, y_tst)\n",
    "        L_tree_test_scores.append(tree_test_score)\n",
    "        print(f\"Tree test score for {i}: {tree_test_score}\")\n",
    "\n",
    "    \"\"\"DecisionTreeClassifier with the best parameters for X_trn_s (automated version)\"\"\"\n",
    "    best_tree_clf = DecisionTreeClassifier(max_features=L8[1], random_state=42)\n",
    "    best_tree_clf.fit(L6[1], y_trn)\n",
    "else:    \n",
    "    \"\"\"DecisionTreeClassifier with the best parameters for X_trn_s (manual version)\"\"\"\n",
    "    best_tree_clf = DecisionTreeClassifier(max_features=4, random_state=42)\n",
    "    best_tree_clf.fit(L6[1], y_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6N9d35nqyjwq"
   },
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9BjxdoryjgR"
   },
   "outputs": [],
   "source": [
    "\"\"\"******************** RandomForestClassifier ********************\"\"\"\n",
    "\n",
    "# \"\"\"RandomForestClassifier without GridSearchCV\"\"\"\n",
    "# L567 = list(zip(L5, L6, L7))\n",
    "# for i, j, k in L567:\n",
    "#     forest_clf = RandomForestClassifier(random_state=42)\n",
    "#     forest_clf.fit(j, y_trn)\n",
    "#     # y_pred = forest_clf.predict(j)\n",
    "#     # forest_mse = mean_squared_error(y_trn, y_pred)\n",
    "#     # forest_rmse = np.sqrt(forest_mse)\n",
    "#     # print(forest_rmse)\n",
    "#     forest_mse_scores = cross_val_score(forest_clf, j, y_trn,\n",
    "#                              scoring=\"neg_mean_squared_error\", cv=8)\n",
    "#     forest_rmse_scores = np.sqrt(-forest_mse_scores)\n",
    "#     # display_scores(tree_rmse_scores)\n",
    "#     print(f\"Mean forest training score for {i}: {forest_rmse_scores.mean()}\")\n",
    "#     forest_test_score = forest_clf.fit(j, y_trn).score(k, y_tst)\n",
    "#     print(f\"Forest test score for {i}: {forest_test_score}\")\n",
    "\n",
    "\"\"\"GridSearchCV for forest without PCA\"\"\"\n",
    "if GridSearch:\n",
    "    for i, j in L12:\n",
    "        param_grid = [\n",
    "            # try 15 (3×5) combinations of hyperparameters\n",
    "            {'n_estimators': [50, 100, 150], 'max_features': [3, 5, 7, 9, 11]},\n",
    "            # then try 15 (3×5) combinations with bootstrap set as False\n",
    "            {'bootstrap': [False], 'n_estimators': [50, 100, 150], 'max_features': [3, 5, 7, 9, 11]},\n",
    "          ]\n",
    "        forest_clf = RandomForestClassifier(random_state=42)\n",
    "        # train across 8 folds, that's a total of (15+15)*8=240 rounds of training\n",
    "        grid_search = GridSearchCV(forest_clf, param_grid, cv=8,\n",
    "                                   scoring='neg_mean_squared_error',\n",
    "                                   return_train_score=True)\n",
    "        grid_search.fit(j, y_trn)\n",
    "        L9.append(grid_search.best_params_['n_estimators'])\n",
    "        L10.append(grid_search.best_params_['max_features'])\n",
    "        print(f\"The best forest parameters for {i}: {grid_search.best_estimator_}\")\n",
    "\n",
    "    \"\"\"GridSearchCV for forest with PCA\"\"\"\n",
    "    for i, j in L34:\n",
    "        param_grid = [\n",
    "            # try 15 (3×5) combinations of hyperparameters\n",
    "            {'n_estimators': [50, 100, 150], 'max_features': [2, 3, 4, 5, 6]},\n",
    "            # then try 15 (3×5) combinations with bootstrap set as False\n",
    "            {'bootstrap': [False], 'n_estimators': [50, 100, 150], 'max_features': [2, 3, 4, 5, 6]},\n",
    "        ]\n",
    "        forest_clf = RandomForestClassifier(random_state=42)\n",
    "        # train across 8 folds, that's a total of (15+15)*8=240 rounds of training\n",
    "        grid_search = GridSearchCV(forest_clf, param_grid, cv=8,\n",
    "                                   scoring='neg_mean_squared_error',\n",
    "                                   return_train_score=True)\n",
    "        grid_search.fit(j, y_trn)\n",
    "        L9.append(grid_search.best_params_['n_estimators'])\n",
    "        L10.append(grid_search.best_params_['max_features'])\n",
    "        print(f\"The best forest parameters for {i}: {grid_search.best_estimator_}\")\n",
    "\n",
    "    \"\"\"RandomForestClassifier after GridSearchCV for all X-variations\"\"\"\n",
    "    L567910 = list(zip(L5, L6, L7, L9, L10))\n",
    "    for i, j, k, l, m in L567910:\n",
    "        forest_clf = RandomForestClassifier(n_estimators=l, max_features=m, random_state=42)\n",
    "        forest_clf.fit(j, y_trn)\n",
    "        # y_pred = forest_clf.predict(j)\n",
    "        # forest_mse = mean_squared_error(y_trn, y_pred)\n",
    "        # forest_rmse = np.sqrt(forest_mse)\n",
    "        # print(forest_rmse)\n",
    "        forest_mse_scores = cross_val_score(forest_clf, j, y_trn,\n",
    "                                 scoring=\"neg_mean_squared_error\", cv=8)\n",
    "        forest_rmse_scores = np.sqrt(-forest_mse_scores)\n",
    "        # display_scores(tree_rmse_scores)\n",
    "        print(f\"\\nMean forest training score for {i}: {forest_rmse_scores.mean()}\")\n",
    "        forest_test_score = forest_clf.fit(j, y_trn).score(k, y_tst)\n",
    "        L_forest_test_scores.append(forest_test_score)\n",
    "        print(f\"Forest test score for {i}: {forest_test_score}\")\n",
    "\n",
    "    \"\"\"RandomForestClassifier with the best parameters for X_trn_s (automated version)\"\"\"\n",
    "    best_forest_clf = RandomForestClassifier(n_estimators=L9[1], max_features=L10[1], random_state=42)\n",
    "    best_forest_clf.fit(L6[1], y_trn)\n",
    "else:\n",
    "    \"\"\"RandomForestClassifier with the best parameters for X_trn_s (manual version)\"\"\"\n",
    "    best_forest_clf = RandomForestClassifier(n_estimators=150, max_features=5, random_state=42)\n",
    "    best_forest_clf.fit(L6[1], y_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPcuZ7-s6bQs"
   },
   "source": [
    "## Ensemble Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHmNAIhN6aIm",
    "outputId": "ecef88f5-d3ab-4d6a-f2c9-37702ab223e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('knn',\n",
       "                              KNeighborsClassifier(algorithm='auto',\n",
       "                                                   leaf_size=30,\n",
       "                                                   metric='minkowski',\n",
       "                                                   metric_params=None,\n",
       "                                                   n_jobs=None, n_neighbors=5,\n",
       "                                                   p=2, weights='uniform')),\n",
       "                             ('mlp',\n",
       "                              MLPClassifier(activation='tanh', alpha=0.0001,\n",
       "                                            batch_size='auto', beta_1=0.9,\n",
       "                                            beta_2=0.999, early_stopping=False,\n",
       "                                            epsilon=1e-08,\n",
       "                                            hidden_layer_sizes=(20, 20),\n",
       "                                            learning_rate='invs...\n",
       "                                  shrinking=True, tol=0.001, verbose=False)),\n",
       "                             ('lr',\n",
       "                              LogisticRegression(C=0.1, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='newton-cg', tol=0.0001,\n",
       "                                                 verbose=0,\n",
       "                                                 warm_start=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 94,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensemble voting ----\n",
    "eclfh = VotingClassifier(estimators=[('knn', best_knn_clf), ('mlp', best_mlp_clf), ('dt', best_tree_clf), ('rf', best_forest_clf), ('svc', best_svc_clf), ('lr', best_logreg_clf)], voting='hard')\n",
    "eclfh.fit(X_trn_s, y_trn)\n",
    "\n",
    "eclfs = VotingClassifier(estimators=[('knn', best_knn_clf), ('mlp', best_mlp_clf), ('dt', best_tree_clf), ('rf', best_forest_clf), ('svc', best_svc_clf), ('lr', best_logreg_clf)], voting='soft')\n",
    "eclfs.fit(X_trn_s, y_trn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODS9vm-k_oKV"
   },
   "source": [
    "## Final scoring with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ntWLoq4_zwx",
    "outputId": "f63cb0a1-be86-47c0-a284-a0af73d56dc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for MLP: 0.628125\n",
      "Score for KNN: 0.60625\n",
      "Score LogReg: 0.6\n",
      "Score for Decision Tree: 0.625\n",
      "Score for Random Forest: 0.690625\n",
      "Score for SVC: 0.59375\n",
      "Score for hard voting classifier: 0.671875\n",
      "Score for soft voting classifier: 0.68125\n"
     ]
    }
   ],
   "source": [
    "# MLP scores\n",
    "# mlp_init.fit(X_trn_s, y_trn)\n",
    "# test_score = mlp_init.score(X_tst_s, y_tst)\n",
    "# print(f\"Score for MLP (init): {test_score}\")\n",
    "test_score = best_mlp_clf.score(X_tst_s, y_tst)\n",
    "print(f\"Score for MLP: {test_score}\")\n",
    "\n",
    "# KNN scores\n",
    "test_score = best_knn_clf.score(X_tst_s, y_tst)\n",
    "print(f\"Score for KNN: {test_score}\")\n",
    "\n",
    "# LogReg scores\n",
    "test_score = best_logreg_clf.score(X_tst_s, y_tst)\n",
    "print(f\"Score LogReg: {test_score}\")\n",
    "\n",
    "# Decision Tree scores\n",
    "test_score = best_tree_clf.score(X_tst_s, y_tst)\n",
    "print(f\"Score for Decision Tree: {test_score}\")\n",
    "\n",
    "# Random Forest Classifier\n",
    "test_score = best_forest_clf.score(X_tst_s, y_tst)\n",
    "print(f\"Score for Random Forest: {test_score}\")\n",
    "\n",
    "# SVC\n",
    "test_score = best_svc_clf.score(X_tst_s, y_tst)\n",
    "print(f\"Score for SVC: {test_score}\")\n",
    "\n",
    "# Ensemble voting\n",
    "test_score = eclfh.score(X_tst_s, y_tst)\n",
    "print(f\"Score for hard voting classifier: {test_score}\")\n",
    "\n",
    "test_score = eclfs.score(X_tst_s, y_tst)\n",
    "print(f\"Score for soft voting classifier: {test_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qr32AYnHO01d"
   },
   "source": [
    "## Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EiD2CLBUO01f",
    "outputId": "23eaae7a-ea2d-489f-d8bf-bd040063128f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality\n",
      " 0         534\n",
      " 1         517\n",
      " 2         160\n",
      "-1          43\n",
      " 3          15\n",
      "-2          10\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "D = pdist(X_trn_s, metric='euclidean')\n",
    "Z = linkage(D, 'average', metric='euclidean')\n",
    "#clustering\n",
    "clus = fcluster(Z,6,criterion='maxclust')+3\n",
    "validation=y_trn-clus\n",
    "vd = pd.DataFrame(validation)\n",
    "print(vd.value_counts())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WineProject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
